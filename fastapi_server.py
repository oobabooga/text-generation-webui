"""
FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è Text Generation WebUI
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç RESTful API endpoints –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
"""

import asyncio
import json
import logging
import sys
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Dict, List, Optional, Any, AsyncGenerator

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
import uvicorn

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º Text Generation WebUI
sys.path.append(str(Path(__file__).parent))

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π Text Generation WebUI
try:
    from modules import shared, models, text_generation, chat
    from modules.models import load_model, unload_model
    from modules.text_generation import generate_reply, stop_everything_event
    from modules.chat import generate_chat_reply
except ImportError as e:
    print(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π: {e}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –∏–∑ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ Text Generation WebUI")
    sys.exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("fastapi_server")

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö Pydantic
class GenerateRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞"""
    prompt: str = Field(..., description="–í—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    max_tokens: int = Field(200, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤")
    temperature: float = Field(0.7, description="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-2.0)")
    top_p: float = Field(0.9, description="Top-p —Å—ç–º–ø–ª–∏–Ω–≥")
    top_k: int = Field(20, description="Top-k —Å—ç–º–ø–ª–∏–Ω–≥")
    repetition_penalty: float = Field(1.18, description="–®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è")
    stream: bool = Field(False, description="–ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è")

class ChatMessage(BaseModel):
    """–°–æ–æ–±—â–µ–Ω–∏–µ –≤ —á–∞—Ç–µ"""
    role: str = Field(..., description="–†–æ–ª—å: user, assistant, system")
    content: str = Field(..., description="–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–æ–æ–±—â–µ–Ω–∏—è")

class ChatRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ —á–∞—Ç"""
    messages: List[ChatMessage] = Field(..., description="–ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π")
    max_tokens: int = Field(200, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤")
    temperature: float = Field(0.7, description="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    stream: bool = Field(False, description="–ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è")

class ModelInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    name: Optional[str] = Field(None, description="–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    loader: Optional[str] = Field(None, description="–¢–∏–ø –∑–∞–≥—Ä—É–∑—á–∏–∫–∞")
    loaded: bool = Field(False, description="–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å")

class GenerateResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é"""
    text: str = Field(..., description="–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç")
    tokens: int = Field(..., description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤")
    generation_time: float = Field(..., description="–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö")

class StatusResponse(BaseModel):
    """–°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–µ—Ä–∞"""
    status: str = Field(..., description="–°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–µ—Ä–∞")
    model: ModelInfo = Field(..., description="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏")
    api_version: str = Field("1.0", description="–í–µ—Ä—Å–∏—è API")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("–ó–∞–ø—É—Å–∫ FastAPI —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è Text Generation WebUI")
    yield
    logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ FastAPI —Å–µ—Ä–≤–µ—Ä–∞")

# –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è FastAPI
app = FastAPI(
    title="Text Generation WebUI FastAPI",
    description="RESTful API –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Text Generation WebUI",
    version="1.0.0",
    lifespan=lifespan
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def get_model_info() -> ModelInfo:
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
    return ModelInfo(
        name=getattr(shared, 'model_name', None),
        loader=getattr(shared.args, 'loader', None),
        loaded=hasattr(shared, 'model') and shared.model is not None
    )

def apply_generation_params(request_data: dict):
    """–ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫ shared –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º"""
    if 'temperature' in request_data:
        shared.settings['temperature'] = request_data['temperature']
    if 'top_p' in request_data:
        shared.settings['top_p'] = request_data['top_p']
    if 'top_k' in request_data:
        shared.settings['top_k'] = request_data['top_k']
    if 'repetition_penalty' in request_data:
        shared.settings['repetition_penalty'] = request_data['repetition_penalty']
    if 'max_tokens' in request_data:
        shared.settings['max_tokens'] = request_data['max_tokens']

# API Endpoints

@app.get("/", response_model=StatusResponse)
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–≤–µ—Ä–µ"""
    return StatusResponse(
        status="running",
        model=get_model_info()
    )

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–µ—Ä–∞"""
    model_info = get_model_info()
    return {
        "status": "healthy" if model_info.loaded else "no_model",
        "timestamp": asyncio.get_event_loop().time()
    }

@app.get("/model/info", response_model=ModelInfo)
async def get_model_information():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    return get_model_info()

@app.post("/model/load")
async def load_model_endpoint(model_name: str, background_tasks: BackgroundTasks):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å"""
    if not model_name:
        raise HTTPException(status_code=400, detail="–ù–µ —É–∫–∞–∑–∞–Ω–æ –∏–º—è –º–æ–¥–µ–ª–∏")
    
    def load_model_task():
        try:
            shared.model_name = model_name
            load_model(model_name)
            logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
    
    background_tasks.add_task(load_model_task)
    return {"message": f"–ù–∞—á–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}"}

@app.post("/model/unload")
async def unload_model_endpoint():
    """–í—ã–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å"""
    try:
        unload_model()
        logger.info("–ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
        return {"message": "–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –≤—ã–≥—Ä—É–∂–µ–Ω–∞"}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")

@app.post("/generate", response_model=GenerateResponse)
async def generate_text(request: GenerateRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ –ø—Ä–æ–º–ø—Ç—É"""
    model_info = get_model_info()
    if not model_info.loaded:
        raise HTTPException(status_code=400, detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    apply_generation_params(request.dict())
    
    try:
        import time
        start_time = time.time()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        result = text_generation.generate_reply(
            prompt=request.prompt,
            state={'max_tokens': request.max_tokens},
            stopping_strings=[],
            is_chat=False,
            escape_html=False
        )
        
        generation_time = time.time() - start_time
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if isinstance(result, tuple) and len(result) > 0:
            generated_text = result[0]
        else:
            generated_text = str(result)
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
        token_count = len(generated_text.split())
        
        return GenerateResponse(
            text=generated_text,
            tokens=token_count,
            generation_time=generation_time
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")

@app.post("/generate/stream")
async def generate_text_stream(request: GenerateRequest):
    """–ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    model_info = get_model_info()
    if not model_info.loaded:
        raise HTTPException(status_code=400, detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    apply_generation_params(request.dict())
    
    async def generate():
        try:
            # –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
            generator = text_generation.generate_reply(
                prompt=request.prompt,
                state={'max_tokens': request.max_tokens},
                stopping_strings=[],
                is_chat=False,
                escape_html=False,
                stream=True
            )
            
            for chunk in generator:
                if isinstance(chunk, tuple) and len(chunk) > 0:
                    text_chunk = chunk[0]
                else:
                    text_chunk = str(chunk)
                
                yield f"data: {json.dumps({'text': text_chunk})}\n\n"
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")

@app.post("/chat")
async def chat_completion(request: ChatRequest):
    """–ß–∞—Ç completion –≤ —Å—Ç–∏–ª–µ OpenAI"""
    model_info = get_model_info()
    if not model_info.loaded:
        raise HTTPException(status_code=400, detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    apply_generation_params(request.dict())
    
    try:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç Text Generation WebUI
        history = {'internal': [], 'visible': []}
        
        for msg in request.messages:
            if msg.role == 'user':
                history['internal'].append([msg.content, ''])
                history['visible'].append([msg.content, ''])
            elif msg.role == 'assistant':
                if history['internal'] and len(history['internal'][-1]) == 2:
                    history['internal'][-1][1] = msg.content
                    history['visible'][-1][1] = msg.content
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_input = request.messages[-1].content if request.messages else ""
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        reply = chat.generate_chat_reply(
            text=user_input,
            history=history,
            state={'max_tokens': request.max_tokens},
            regenerate=False,
            _continue=False,
            loading_message=True
        )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
        if isinstance(reply, tuple) and len(reply) > 1:
            response_text = reply[1]['visible'][-1][1] if reply[1]['visible'] else ""
        else:
            response_text = str(reply)
        
        return {
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(user_input.split()),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(user_input.split()) + len(response_text.split())
            }
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")

@app.post("/stop")
async def stop_generation():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ–∫—É—â—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é"""
    try:
        stop_everything_event.set()
        return {"message": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: {e}")

@app.get("/settings")
async def get_settings():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    return shared.settings

@app.post("/settings")
async def update_settings(settings: Dict[str, Any]):
    """–û–±–Ω–æ–≤–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        for key, value in settings.items():
            if key in shared.settings:
                shared.settings[key] = value
        return {"message": "–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã", "updated": settings}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}")

if __name__ == "__main__":
    import sys
    
    # –û—á–∏—â–∞–µ–º sys.argv —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
    original_argv = sys.argv.copy()
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—à–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    filtered_argv = [original_argv[0]]  # –û—Å—Ç–∞–≤–ª—è–µ–º –∏–º—è —Å–∫—Ä–∏–ø—Ç–∞
    
    # –ò—â–µ–º –Ω–∞—à–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    for i, arg in enumerate(original_argv[1:], 1):
        if arg in ["--host", "--port", "--reload"] or (i > 1 and original_argv[i-1] in ["--host", "--port"]):
            filtered_argv.append(arg)
    
    sys.argv = filtered_argv
    
    import argparse
    
    parser = argparse.ArgumentParser(description="FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è Text Generation WebUI")
    parser.add_argument("--host", type=str, default="127.0.0.1", help="–•–æ—Å—Ç –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞")
    parser.add_argument("--port", type=int, default=8000, help="–ü–æ—Ä—Ç –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞")
    parser.add_argument("--reload", action="store_true", help="–ê–≤—Ç–æ–ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö")
    
    args = parser.parse_args()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ FastAPI —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è Text Generation WebUI")
    print("=" * 60)
    print(f"üåê –°–µ—Ä–≤–µ—Ä –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://{args.host}:{args.port}")
    print(f"üìñ API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://{args.host}:{args.port}/docs")
    print(f"üîÑ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π API: http://{args.host}:{args.port}/redoc")
    print("=" * 60)
    
    logger.info(f"–ó–∞–ø—É—Å–∫ FastAPI —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ {args.host}:{args.port}")
    
    try:
        uvicorn.run(
            "fastapi_server:app",
            host=args.host,
            port=args.port,
            reload=args.reload,
            log_level="info"
        )
    finally:
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
        sys.argv = original_argv