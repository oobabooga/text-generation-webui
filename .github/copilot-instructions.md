# Инструкции GitHub Copilot для Text Generation Web UI

## Обзор проекта

Это веб-интерфейс на основе Gradio для локального запуска больших языковых моделей с поддержкой множественных бэкендов (llama.cpp, Transformers, ExLlama, TensorRT-LLM). Кодовая база следует модульной архитектуре с четким разделением между UI, загрузкой моделей, генерацией текста и системами расширений.

## Ключевые компоненты архитектуры

### Точки входа и поток выполнения
- **Основной сервер**: `server.py` - главная точка входа, инициализирует Gradio, загружает расширения и создает веб-интерфейс
- **Управление моделями**: `modules/models.py` + `modules/loaders.py` - автоматически определяет типы моделей и обрабатывает загрузку через различные бэкенды
- **Ядро генерации**: `modules/text_generation.py` - центральная функция `generate_reply()` с потокобезопасностью через `generation_lock`
- **Структура UI**: разделена между файлами `modules/ui_*.py` (чат, параметры, меню модели и т.д.) с общим состоянием в `modules/shared.py`

### Критическое управление состоянием
- **Глобальное состояние**: `modules/shared.py` содержит все общие переменные (`model`, `tokenizer`, `gradio`, `args`)
- **Потокобезопасность**: генерация использует блокировки для предотвращения конфликтов; UI автосохраняет настройки с дебаунсингом
- **Состояние расширений**: каждое расширение поддерживает отдельное состояние в словаре состояний `modules/extensions.py`

### Система бэкендов моделей
Система загрузчиков в `modules/loaders.py` определяет параметры для каждого бэкенда:
- **llama.cpp**: файлы GGUF, слои GPU, размер контекста, стриминг
- **Transformers**: модели HuggingFace, квантизация, управление памятью
- **ExLlama**: оптимизированные движки вывода с пользовательскими типами кэша
- Автоопределение на основе типов файлов и метаданных модели

## Рабочие процессы разработчика

### Настройка среды разработки
```bash
# Портативная установка (быстрее всего для GGUF моделей)
pip install -r requirements/portable/requirements.txt
python server.py --portable --api --auto-launch

# Полная установка для всех бэкендов
./start_windows.bat  # Создает conda окружение в installer_files/
```

### Разработка расширений
- Расширения размещаются в `extensions/NAME/script.py` или `user_data/extensions/NAME/script.py` (пользовательские имеют приоритет)
- Должны реализовать `setup()` для инициализации и могут определять UI функции
- Используют словарь параметров расширения для настроек: `extension.params['setting_name']`
- Подключаются к генерации через систему callback'ов `apply_extensions()`

### Интеграция моделей
- Модели хранятся в `user_data/models/`
- Метаданные кэшируются в `user_data/models/config.yaml`
- LoRA адаптеры в `user_data/loras/`
- Определения персонажей в `user_data/characters/` (формат JSON)

### Тестирование и отладка
- Используйте флаг `--verbose` для просмотра всех промптов в терминале
- Отладка загрузки моделей с `--model-menu` для интерактивного выбора
- Проверяйте `user_data/logs/` для подробных логов ошибок
- API доступно через флаг `--api` для тестирования эндпоинтов

## Специфичные для проекта паттерны

### Структура UI компонентов
```python
# Компоненты Gradio хранятся в словаре shared.gradio
shared.gradio['component_name'] = gr.Component()

# Темы и CSS загружаются из директории css/
# JavaScript функциональность разделена по файлам js/
```

### Хуки расширений
Расширения могут изменять:
- **Обработку ввода**: Модификация промпта до генерации
- **Обработку вывода**: Манипуляция текста после генерации
- **Элементы UI**: Добавление пользовательских вкладок, параметров, кнопок
- **API эндпоинты**: Пользовательские маршруты, совместимые с OpenAI

### Особенности системы чата
- Использует шаблоны Jinja2 для форматирования промптов (`user_data/instruction-templates/`)
- История чата хранится как JSON с отслеживанием метаданных
- Поддерживает ветвление разговоров и редактирование сообщений
- Карты персонажей определяют личность, приветствие и контекст

### Паттерны конфигурации
- Аргументы командной строки определены в парсере `modules/shared.py`
- Настройки автосохраняются в `user_data/settings.yaml`
- Флаги сессии в `user_data/CMD_FLAGS.txt`
- Конфигурации моделей в директориях моделей

## Точки интеграции

### Совместимость с OpenAI API
- Эндпоинты `/v1/chat/completions` и `/v1/completions` в `extensions/openai/`
- Поддержка стриминга с SSE
- Поддержка вызова инструментов для функций

### Многобэкендная загрузка моделей
- Логика выбора бэкенда в `modules/models.py:load_model()`
- Каждый загрузчик обрабатывает собственное сопоставление параметров
- Автоматическое определение слоев GPU для GGUF моделей
- Управление памятью между GPU/CPU/диском

### Система обучения
- Обучение LoRA/QLoRA в `modules/training.py`
- Форматирование датасета для instruction tuning
- Мониторинг прогресса обучения через интерфейс Gradio
- Слияние и сохранение адаптеров

## Частые подводные камни

- **Загрузка модели**: Всегда проверяйте, загружена ли модель, прежде чем перезагружать
- **Потокобезопасность**: Используйте `generation_lock` для любых операций с моделью
- **Загрузка расширений**: Пользовательские расширения переопределяют системные с тем же именем
- **Сохранение настроек**: Изменения UI автосохраняются, но могут потребовать ручного обновления
- **Управление памятью**: Следите за использованием VRAM, особенно при настройке нескольких GPU