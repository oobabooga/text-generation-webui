#!/usr/bin/env python3
"""
FastAPI Proxy Server –¥–ª—è Text Generation WebUI
–ü—Ä–æ–∫—Å–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ endpoint'–∞–º–∏
"""

import asyncio
import json
import time
from typing import Optional, Dict, Any, List
import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
import uvicorn
import argparse

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: Optional[int] = 100
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9
    stop: Optional[List[str]] = None

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    max_tokens: Optional[int] = 100
    temperature: Optional[float] = 0.7
    stream: Optional[bool] = False

class HealthResponse(BaseModel):
    status: str
    timestamp: float
    proxy_mode: bool = True
    backend_status: str

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Text Generation WebUI - FastAPI Proxy",
    description="–ü—Ä–æ–∫—Å–∏ —Å–µ—Ä–≤–µ—Ä –¥–ª—è Text Generation WebUI —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ endpoint'–∞–º–∏",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BACKEND_URL = "http://127.0.0.1:5000"
client = httpx.AsyncClient(timeout=60.0)

@app.on_event("startup")
async def startup_event():
    print("üöÄ FastAPI Proxy Server –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    print(f"üì° –ë—ç–∫–µ–Ω–¥: {BACKEND_URL}")

@app.on_event("shutdown")
async def shutdown_event():
    await client.aclose()
    print("üëã FastAPI Proxy Server –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint"""
    return {
        "message": "Text Generation WebUI - FastAPI Proxy",
        "version": "1.0.0",
        "endpoints": {
            "health": "/health",
            "generate": "/generate", 
            "chat": "/chat",
            "openai_proxy": "/v1/chat/completions",
            "docs": "/docs"
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–µ—Ä
        response = await client.get(f"{BACKEND_URL}/v1/models", timeout=5.0)
        backend_status = "ready" if response.status_code == 200 else "error"
    except Exception:
        backend_status = "unavailable"
    
    return HealthResponse(
        status="ready" if backend_status in ["ready"] else "error",
        timestamp=time.time(),
        proxy_mode=True,
        backend_status=backend_status
    )

@app.post("/generate")
async def generate_text(request: GenerateRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç–æ–π prompt"""
    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI API
        openai_request = {
            "model": "gpt-3.5-turbo",
            "messages": [
                {"role": "user", "content": request.prompt}
            ],
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "stop": request.stop
        }
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–µ—Ä–≤–µ—Ä—É
        response = await client.post(
            f"{BACKEND_URL}/v1/chat/completions",
            json=openai_request,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            result = response.json()
            if "choices" in result and result["choices"]:
                generated_text = result["choices"][0]["message"]["content"]
                return {
                    "response": generated_text,
                    "status": "success",
                    "prompt": request.prompt,
                    "timestamp": time.time()
                }
            else:
                raise HTTPException(status_code=500, detail="–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞")
        else:
            raise HTTPException(status_code=response.status_code, detail=f"–û—à–∏–±–∫–∞ –±—ç–∫–µ–Ω–¥–∞: {response.text}")
            
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="–ë—ç–∫–µ–Ω–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}")

@app.post("/chat")
async def chat_completion(request: ChatRequest):
    """Chat completion endpoint"""
    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI API
        openai_request = {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": msg.role, "content": msg.content} for msg in request.messages],
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "stream": request.stream
        }
        
        response = await client.post(
            f"{BACKEND_URL}/v1/chat/completions",
            json=openai_request,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            result = response.json()
            if "choices" in result and result["choices"]:
                generated_text = result["choices"][0]["message"]["content"]
                return {
                    "response": generated_text,
                    "status": "success",
                    "messages": len(request.messages),
                    "timestamp": time.time()
                }
            else:
                raise HTTPException(status_code=500, detail="–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞")
        else:
            raise HTTPException(status_code=response.status_code, detail=f"–û—à–∏–±–∫–∞ –±—ç–∫–µ–Ω–¥–∞: {response.text}")
            
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="–ë—ç–∫–µ–Ω–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}")

@app.post("/v1/chat/completions")
async def openai_proxy(request: Request):
    """–ü—Ä–æ–∫—Å–∏ –¥–ª—è OpenAI API —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    try:
        body = await request.body()
        
        response = await client.post(
            f"{BACKEND_URL}/v1/chat/completions",
            content=body,
            headers={"Content-Type": "application/json"}
        )
        
        return JSONResponse(
            content=response.json() if response.status_code == 200 else {"error": response.text},
            status_code=response.status_code
        )
        
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="–ë—ç–∫–µ–Ω–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏: {str(e)}")

@app.get("/info")
async def server_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–≤–µ—Ä–µ"""
    return {
        "server": "FastAPI Proxy for Text Generation WebUI",
        "version": "1.0.0",
        "proxy_mode": True,
        "backend": BACKEND_URL,
        "timestamp": time.time(),
        "features": [
            "OpenAI API Proxy",
            "Simple Generate Endpoint", 
            "Chat Completion",
            "Health Monitoring",
            "CORS Support"
        ]
    }

def main():
    parser = argparse.ArgumentParser(description="FastAPI Proxy Server")
    parser.add_argument("--port", type=int, default=8000, help="–ü–æ—Ä—Ç —Å–µ—Ä–≤–µ—Ä–∞")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="–•–æ—Å—Ç —Å–µ—Ä–≤–µ—Ä–∞")
    parser.add_argument("--backend", type=str, default="http://127.0.0.1:5000", help="URL –±—ç–∫–µ–Ω–¥–∞")
    args = parser.parse_args()
    
    global BACKEND_URL
    BACKEND_URL = args.backend
    
    print(f"üöÄ –ó–∞–ø—É—Å–∫ FastAPI Proxy Server –Ω–∞ {args.host}:{args.port}")
    print(f"üì° –ü—Ä–æ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∫: {BACKEND_URL}")
    
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info"
    )

if __name__ == "__main__":
    main()